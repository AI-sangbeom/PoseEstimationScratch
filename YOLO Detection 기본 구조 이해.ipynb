{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a37b7d3",
   "metadata": {},
   "source": [
    "# YOLO 기본 구조 이해\n",
    "- Backbone: CNN으로 이미지 특징 추출 \n",
    "- Head: 각 feature map pixel마다 \"box/class\" 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d843a",
   "metadata": {},
   "source": [
    "## Backbone : 특징 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c36f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class YOLOBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YOLOBackbone, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3), # 448→224\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                                   # 224→112\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                                   # 112→56\n",
    "            nn.Conv2d(192, 128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                                   # 56→28\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                                   # 28→14\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, stride=2, padding=1), # 14→7\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.features(x)  # [batch, 1024, 7, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backbone 사용 예시\n",
    "input_image = torch.randn(1, 3, 448, 448)\n",
    "\n",
    "backbone = YOLOBackbone()\n",
    "feature_map = backbone(input_image)\n",
    "print(feature_map.shape)  # torch.Size([1, 1024, 7, 7])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf760b",
   "metadata": {},
   "source": [
    "## Head : 예측 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66576da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOHead(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YOLOHead, self).__init__()\n",
    "        # s : grid size\n",
    "        # b : number of boxes (후보군 개수 설정)\n",
    "        # c : number of classes\n",
    "        self.S, self.B, self.C = S, B, C\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * S * S, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, S * S * (B * 5 + C)),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)  \n",
    "        x = x.view(-1, self.S, self.S, self.B * 5 + self.C)\n",
    "        return x  # [batch, 7, 7, 30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c596420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head 사용 예시\n",
    "head = YOLOHead(S=7, B=2, C=20)\n",
    "pred_tensor = head(feature_map)\n",
    "print(pred_tensor.shape)  # torch.Size([1, 7, 7, 30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f89648",
   "metadata": {},
   "source": [
    "## Pose-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def yolo_postprocess(pred_tensor, S=7, B=2, C=20, conf_thresh=0.2, nms_thresh=0.5, img_size=448):\n",
    "    \"\"\"\n",
    "    pred_tensor: [S, S, B*5 + C] (ex: [7,7,30])\n",
    "    Returns: 최종 bounding box 리스트 [x1, y1, x2, y2, score, class]\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    pred_tensor = pred_tensor.squeeze().detach().cpu().numpy()  # [7, 7, 30]\n",
    "    for i in range(S):\n",
    "        for j in range(S):\n",
    "            cell = pred_tensor[i, j]\n",
    "            class_probs = cell[B*5:]\n",
    "            best_class = np.argmax(class_probs)\n",
    "            best_class_score = class_probs[best_class]\n",
    "            for b in range(B):\n",
    "                x, y, w, h, conf = cell[b*5:(b+1)*5]\n",
    "                cx = (j + x) / S * img_size\n",
    "                cy = (i + y) / S * img_size\n",
    "                bw = w * img_size\n",
    "                bh = h * img_size\n",
    "                x1 = int(cx - bw/2)\n",
    "                y1 = int(cy - bh/2)\n",
    "                x2 = int(cx + bw/2)\n",
    "                y2 = int(cy + bh/2)\n",
    "                score = conf * best_class_score\n",
    "                if score > conf_thresh:\n",
    "                    boxes.append([x1, y1, x2, y2, score, best_class])\n",
    "    boxes = np.array(boxes)\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    keep = nms(boxes, nms_thresh)\n",
    "    return boxes[keep]\n",
    "\n",
    "def nms(boxes, iou_thresh):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    indices = boxes[:, 4].argsort()[::-1]\n",
    "    keep = []\n",
    "    while len(indices) > 0:\n",
    "        i = indices[0]\n",
    "        keep.append(i)\n",
    "        others = indices[1:]\n",
    "        ious = compute_iou(boxes[i], boxes[others])\n",
    "        indices = others[ious < iou_thresh]\n",
    "    return keep\n",
    "\n",
    "def compute_iou(box, boxes):\n",
    "    if len(boxes) == 0:\n",
    "        return np.array([])\n",
    "    x1 = np.maximum(box[0], boxes[:,0])\n",
    "    y1 = np.maximum(box[1], boxes[:,1])\n",
    "    x2 = np.minimum(box[2], boxes[:,2])\n",
    "    y2 = np.minimum(box[3], boxes[:,3])\n",
    "    inter = np.maximum(x2-x1, 0) * np.maximum(y2-y1, 0)\n",
    "    area1 = (box[2]-box[0]) * (box[3]-box[1])\n",
    "    area2 = (boxes[:,2]-boxes[:,0]) * (boxes[:,3]-boxes[:,1])\n",
    "    union = area1 + area2 - inter\n",
    "    iou = inter / (union + 1e-6)\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a241c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 입력\n",
    "input_image = torch.randn(1, 3, 448, 448)\n",
    "\n",
    "# 2) Backbone\n",
    "backbone = YOLOBackbone()\n",
    "feature_map = backbone(input_image)\n",
    "\n",
    "# 3) Head\n",
    "head = YOLOHead(S=7, B=4, C=20)\n",
    "pred_tensor = head(feature_map)\n",
    "\n",
    "# 4) 후처리\n",
    "boxes = yolo_postprocess(pred_tensor[0], S=7, B=4, C=20, conf_thresh=0.2, nms_thresh=0.5, img_size=448)\n",
    "print(\"최종 바운딩 박스:\", boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd66a8",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc94d4c",
   "metadata": {},
   "source": [
    "## Dataset 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682c227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "VOC_CLASSES = [\n",
    "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
    "    \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
    "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]\n",
    "CLASS2IDX = {cls: i for i, cls in enumerate(VOC_CLASSES)}\n",
    "\n",
    "def voc_to_yolo_target(objects, img_w, img_h, S=7, B=2, C=20):\n",
    "    target = np.zeros((S, S, B * 5 + C), dtype=np.float32)\n",
    "    for obj in objects:\n",
    "        cls_idx = CLASS2IDX[obj['name']]\n",
    "        xmin, ymin, xmax, ymax = obj['bbox']\n",
    "        # x, y, w, h 생성 (norm 값)\n",
    "        cx = (xmin + xmax) / 2 / img_w\n",
    "        cy = (ymin + ymax) / 2 / img_h\n",
    "        bw = (xmax - xmin) / img_w\n",
    "        bh = (ymax - ymin) / img_h\n",
    "        # grid 좌표로 변환\n",
    "        i = int(cy * S)\n",
    "        j = int(cx * S)\n",
    "        x_cell = cx * S - j\n",
    "        y_cell = cy * S - i\n",
    "        for b in range(B):\n",
    "            if target[i, j, b*5+4] == 0:\n",
    "                target[i, j, b*5:b*5+5] = [x_cell, y_cell, bw, bh, 1.0]\n",
    "                break\n",
    "        target[i, j, B*5 + cls_idx] = 1.0\n",
    "    return target   # shape: (S, S, B*5+C)\n",
    "\n",
    "def get_voc_objects(target):\n",
    "    size = target['annotation']['size']\n",
    "    img_w, img_h = int(size['width']), int(size['height'])\n",
    "    objs = target['annotation']['object']\n",
    "    if not isinstance(objs, list):\n",
    "        objs = [objs]\n",
    "    objects = []\n",
    "    for obj in objs:\n",
    "        name = obj['name']\n",
    "        bbox = [int(obj['bndbox']['xmin']),\n",
    "                int(obj['bndbox']['ymin']),\n",
    "                int(obj['bndbox']['xmax']),\n",
    "                int(obj['bndbox']['ymax'])]\n",
    "        objects.append({\"name\": name, \"bbox\": bbox})\n",
    "    return objects, img_w, img_h\n",
    "\n",
    "def preprocess_and_save_voc_yolo(root, save_dir, year='2012', image_set='train', S=7, B=2, C=20, img_size=448):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    dataset = VOCDetection(root=root, year=year, image_set=image_set, download=True)\n",
    "    transform = T.Compose([\n",
    "        T.Resize((img_size, img_size)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    img_save_dir = os.path.join(save_dir, 'images')\n",
    "    label_save_dir = os.path.join(save_dir, 'labels')\n",
    "    os.makedirs(img_save_dir, exist_ok=True)\n",
    "    os.makedirs(label_save_dir, exist_ok=True)\n",
    "    \n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        img, target = dataset[idx]\n",
    "        img_t = transform(img) # torch.Tensor (3, 448, 448)\n",
    "        objects, img_w, img_h = get_voc_objects(target)\n",
    "        yolo_target = voc_to_yolo_target(objects, img_w, img_h, S, B, C)\n",
    "        # 저장 (이미지: .pt, 라벨: .npy)\n",
    "        torch.save(img_t, os.path.join(img_save_dir, f\"{idx:06d}.pt\"))\n",
    "        np.save(os.path.join(label_save_dir, f\"{idx:06d}.npy\"), yolo_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import VOCDetection\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "class CachedVOCYOLODataset(Dataset):\n",
    "    def __init__(self, cache_dir, S=7, B=2, C=20):\n",
    "        self.img_dir = os.path.join(cache_dir, 'images')\n",
    "        self.label_dir = os.path.join(cache_dir, 'labels')\n",
    "        self.indices = sorted([\n",
    "            fname.replace('.pt', '')\n",
    "            for fname in os.listdir(self.img_dir)\n",
    "            if fname.endswith('.pt')\n",
    "        ])\n",
    "        self.S, self.B, self.C = S, B, C\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_str = self.indices[idx]\n",
    "        img = torch.load(os.path.join(self.img_dir, f\"{idx_str}.pt\"))\n",
    "        label = np.load(os.path.join(self.label_dir, f\"{idx_str}.npy\"))\n",
    "        label = torch.from_numpy(label)\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5deceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cached_dataset = CachedVOCYOLODataset('/media/otter/hard_otter/VOC', S=7, B=2, C=20)\n",
    "loader = DataLoader(cached_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "for imgs, targets in loader:\n",
    "    print(\"이미지 배치 shape:\", imgs.shape)\n",
    "    print(\"라벨 배치 shape:\", targets.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c03b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv1Simple(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YOLOv1Simple, self).__init__()\n",
    "        self.backbone = YOLOBackbone()\n",
    "        self.head = YOLOHead(S=S, B=B, C=C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "def yolo_loss(pred, target, S=7, B=2, C=20, lambda_coord=5, lambda_noobj=0.5):\n",
    "    '''\n",
    "    pred, target: [batch, S, S, B*5 + C]\n",
    "    '''\n",
    "    # 마스킹\n",
    "    conf_idx = [b*5+4 for b in range(B)]\n",
    "    # 각 박스(conf) 마다 마스크 만들고 stack\n",
    "    obj_mask = torch.stack([target[..., idx] > 0 for idx in conf_idx], dim=-1)   # [batch, 7, 7, B]\n",
    "    noobj_mask = torch.stack([target[..., idx] == 0 for idx in conf_idx], dim=-1)\n",
    "\n",
    "    # print(obj_mask.shape)\n",
    "    # ========== (1) Localization Loss (x, y, w, h) ==========\n",
    "    # 좌표, width, height는 오직 객체가 있을 때만 loss\n",
    "    loc_loss = 0.0\n",
    "    for b in range(B):\n",
    "        # (x, y) loss\n",
    "        loc_loss += torch.sum(\n",
    "            obj_mask[..., b] * (\n",
    "                (pred[..., b*5+0] - target[..., b*5+0])**2 +\n",
    "                (pred[..., b*5+1] - target[..., b*5+1])**2\n",
    "            )\n",
    "        )\n",
    "        # (w, h) loss (sqrt로)\n",
    "        loc_loss += torch.sum(\n",
    "            obj_mask[..., b] * (\n",
    "                (torch.sqrt(torch.abs(pred[..., b*5+2]+1e-6)) - torch.sqrt(target[..., b*5+2]+1e-6))**2 +\n",
    "                (torch.sqrt(torch.abs(pred[..., b*5+3]+1e-6)) - torch.sqrt(target[..., b*5+3]+1e-6))**2\n",
    "            )\n",
    "        )\n",
    "\n",
    "    loc_loss = lambda_coord * loc_loss\n",
    "\n",
    "    # ========== (2) Confidence Loss ==========\n",
    "    conf_loss = 0.0\n",
    "    for b in range(B):\n",
    "        # object (conf) loss\n",
    "        conf_loss += torch.sum(\n",
    "            obj_mask[..., b] * (pred[..., b*5+4] - target[..., b*5+4])**2\n",
    "        )\n",
    "        # no-object (conf) loss\n",
    "        conf_loss += lambda_noobj * torch.sum(\n",
    "            noobj_mask[..., b] * (pred[..., b*5+4] - target[..., b*5+4])**2\n",
    "        )\n",
    "\n",
    "    # ========== (3) Class Loss ==========\n",
    "    # 객체가 있는 셀만 class loss (B개 중 첫 박스만 사용, 보통)\n",
    "    # target[..., B*5+B*5:] => [batch, S, S, C]\n",
    "    class_loss = torch.sum(\n",
    "        (target[..., B*5+0:B*5+C] - pred[..., B*5+0:B*5+C])**2 * obj_mask[..., 0].unsqueeze(-1)\n",
    "    )\n",
    "\n",
    "    # ========== (4) 합계 ==========\n",
    "    total_loss = loc_loss + conf_loss + class_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6939f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "def plot_loss_curve(loss_history):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    plt.plot(loss_history, label='Avg Loss')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.legend()\n",
    "    fig.canvas.draw()\n",
    "    saveimg = np.asarray(fig.canvas.buffer_rgba())\n",
    "    saveimg = saveimg[..., :3]  # 알파 채널 제거\n",
    "    saveimg = cv2.cvtColor(saveimg, cv2.COLOR_RGB2BGR)    \n",
    "    cv2.imshow('loss_curve', saveimg)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "def show_yolo_cv2(img, pred_tensor, class_names, conf_thresh=0.2, nms_thresh=0.5, img_size=448, winname=\"YOLO Detection\"):\n",
    "    # 후처리로 박스 추출\n",
    "    boxes = yolo_postprocess(pred_tensor, S=7, B=2, C=20, conf_thresh=conf_thresh, nms_thresh=nms_thresh, img_size=img_size)\n",
    "    \n",
    "    # Tensor → numpy 변환 및 이미지 정규화 해제\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.permute(1,2,0).cpu().numpy()\n",
    "    if img.max() <= 1.0:\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "    else:\n",
    "        img = img.astype(np.uint8)\n",
    "    \n",
    "    # RGB → BGR (cv2는 BGR)\n",
    "    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # bbox 그리기\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2, score, cls_idx = box\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "        color = (0,0,255)\n",
    "        cv2.rectangle(img_bgr, (x1, y1), (x2, y2), color, 2)\n",
    "        label = f\"{class_names[int(cls_idx)]}: {score:.2f}\"\n",
    "        cv2.putText(img_bgr, label, (x1, max(0, y1-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "    \n",
    "    # 이미지 띄우기\n",
    "    cv2.imshow(winname, img_bgr)\n",
    "    cv2.waitKey(1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d8d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm \n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "model = YOLOv1Simple(S=7, B=2, C=20).to('cuda')\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=1e-3,            # (권장 시작값: 1e-3 ~ 1e-2, 후에 줄여도 됨)\n",
    "    momentum=0.9,       # 일반적으로 0.9~0.95 사용\n",
    "    weight_decay=5e-4   # YOLOv1 논문 기본값, 필요 없으면 0으로\n",
    ")\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # 50 epoch마다 lr x0.1\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0    \n",
    "    pbar = tqdm.tqdm(loader)\n",
    "    for i, (imgs, targets) in enumerate(pbar):\n",
    "        imgs, targets = imgs.to('cuda'), targets.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "        loss = yolo_loss(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        total_samples += imgs.size(0)\n",
    "\n",
    "        # 평균 loss\n",
    "        avg_loss = running_loss / total_samples if total_samples > 0 else 0.0\n",
    "        pbar.desc = f'Epoch {epoch}, Avg Loss: {avg_loss:.4f}'\n",
    "        if i%30==0:\n",
    "            img_vis = imgs[0].detach().cpu()\n",
    "            pred_vis = preds[0].detach().cpu()\n",
    "            show_yolo_cv2(img_vis, pred_vis, VOC_CLASSES, winname=\"YOLO Detection\")\n",
    "    loss_history.append(avg_loss)\n",
    "    plot_loss_curve(loss_history)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b09e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "VOC_CLASSES = [\n",
    "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
    "    \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
    "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]\n",
    "\n",
    "def plot_boxes_on_image(image, boxes, class_names=VOC_CLASSES):\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.permute(1, 2, 0).cpu().numpy()  # [H,W,C]\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    ax.imshow(image.astype(np.uint8) if image.max() > 1.5 else (image * 255).astype(np.uint8))\n",
    "    print(boxes)\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2, score, cls_idx = box\n",
    "        print(box)\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2 - x1, y2 - y1,\n",
    "            linewidth=2, edgecolor='red', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1 - 3, f\"{class_names[int(cls_idx)]}: {score:.2f}\",\n",
    "                color='yellow', fontsize=10, weight='bold',\n",
    "                bbox=dict(facecolor='red', alpha=0.5, edgecolor='none'))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad55de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 예시: DataLoader에서 하나 뽑아서 바로 실행\n",
    "for img, _ in loader:\n",
    "    img0 = img[0].cuda()  # [3, 448, 448]\n",
    "    with torch.no_grad():\n",
    "        pred_tensor = model(img0.unsqueeze(0))[0]  # [7,7,30]\n",
    "    boxes = yolo_postprocess(pred_tensor, S=7, B=2, C=20, conf_thresh=0.2, nms_thresh=0.5, img_size=448)\n",
    "    plot_boxes_on_image(img0, boxes, VOC_CLASSES)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd51624",
   "metadata": {},
   "source": [
    "# YOLOv5 Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b391a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. Focus Layer (YOLOv5만의 최초 특징 추출)\n",
    "class Focus(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=32):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels * 4, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "    def forward(self, x):\n",
    "        # 슬라이싱으로 4분할 채널 확장\n",
    "        patch1 = x[..., ::2, ::2]\n",
    "        patch2 = x[..., ::2, 1::2]\n",
    "        patch3 = x[..., 1::2, ::2]\n",
    "        patch4 = x[..., 1::2, 1::2]\n",
    "        x = torch.cat([patch1, patch2, patch3, patch4], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "# 2. Basic Conv Block\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k=3, s=1, p=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, k, s, p, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn(self.conv(x)))\n",
    "        return x\n",
    "\n",
    "# 3. CSPBlock (간단화)\n",
    "class CSPBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n=1):\n",
    "        super().__init__()\n",
    "        self.part1 = nn.Sequential(*[ConvBlock(in_channels//2, in_channels//2) for _ in range(n)])\n",
    "        self.part2 = nn.Identity()\n",
    "        self.conv_cat = ConvBlock(in_channels, out_channels, k=1, s=1, p=0)\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x.chunk(2, dim=1)  # channel split\n",
    "        x1 = self.part1(x1)\n",
    "        x2 = self.part2(x2)\n",
    "        x_cat = torch.cat([x1, x2], dim=1)\n",
    "        x_out = self.conv_cat(x_cat)\n",
    "        return x_out\n",
    "\n",
    "# 4. SPP Module (Spatial Pyramid Pooling)\n",
    "class SPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBlock(in_channels, in_channels//2, k=1, s=1, p=0)\n",
    "        self.poolings = nn.ModuleList([\n",
    "            nn.MaxPool2d(kernel_size=ks, stride=1, padding=ks//2) for ks in [5, 9, 13]\n",
    "        ])\n",
    "        self.conv2 = ConvBlock(in_channels*2, out_channels, k=1, s=1, p=0)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        pool_outs = [x] + [pool(x) for pool in self.poolings]\n",
    "        x = torch.cat(pool_outs, dim=1)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "# 5. Neck (FPN+PANet 간단화)\n",
    "class Neck(nn.Module):\n",
    "    def __init__(self, c3, c4, c5):\n",
    "        super().__init__()\n",
    "        # 채널 조정, 업샘플링/다운샘플링\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv_c5 = ConvBlock(c5, c4, k=1, s=1, p=0)\n",
    "        self.conv_c4 = ConvBlock(c4*2, c3, k=1, s=1, p=0)\n",
    "        self.downsample = nn.Conv2d(c3, c4, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x3, x4, x5):\n",
    "        p5 = self.upsample(self.conv_c5(x5))     # [B, c4, 32, 32]\n",
    "        p4 = torch.cat([p5, x4], dim=1)          # [B, c4+c4, 32, 32]\n",
    "        p4 = self.conv_c4(p4)                    # [B, c3, 32, 32]\n",
    "        p3 = self.upsample(p4)                   # [B, c3, 64, 64]\n",
    "        p3 = torch.cat([p3, x3], dim=1)          # [B, c3+x3의채널, 64, 64]\n",
    "        return p3, p4, p5\n",
    "\n",
    "\n",
    "# class SimpleYOLOHead(nn.Module):\n",
    "#     def __init__(self, in_channels, anchors, S, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.anchors = anchors\n",
    "#         self.S = S\n",
    "#         self.num_classes = num_classes\n",
    "#         self.conv = nn.Conv2d(in_channels, len(anchors) * (5 + num_classes), 1)\n",
    "#     def forward(self, x):\n",
    "#         B = x.size(0)\n",
    "#         out = self.conv(x)  # (B, A*(5+C), S, S)\n",
    "#         out = out.view(B, len(self.anchors), 5 + self.num_classes, self.S, self.S)\n",
    "#         out = out.permute(0, 1, 3, 4, 2)  # (B, A, S, S, 5+C)\n",
    "#         return out\n",
    "\n",
    "# 6. Detection Head (각 스케일별 예측, 간단화)\n",
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, in_channels, anchors, S, num_classes):\n",
    "        super().__init__()\n",
    "        self.anchors = anchors\n",
    "        self.S = S\n",
    "        self.num_classes = num_classes\n",
    "        self.conv = nn.Conv2d(in_channels, len(anchors) * (5 + num_classes), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        out = self.conv(x)\n",
    "        out = out.view(B, len(self.anchors), 5 + self.num_classes, self.S, self.S)\n",
    "        out = out.permute(0, 1, 3, 4, 2)  # (B, A, S, S, 5+C)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 7. 전체 YOLOv5 구조 (연결)\n",
    "class YOLOv5Demo(nn.Module):\n",
    "    def __init__(self, num_classes=80):\n",
    "        super().__init__()\n",
    "        self.focus = Focus(3, 32)\n",
    "        self.conv1 = ConvBlock(32, 64, s=2)\n",
    "        self.csp1 = CSPBlock(64, 128, n=1)\n",
    "        self.conv2 = ConvBlock(128, 256, s=2)\n",
    "        self.csp2 = CSPBlock(256, 256, n=1)\n",
    "        self.conv3 = ConvBlock(256, 512, s=2)\n",
    "        self.spp = SPP(512, 512)\n",
    "        self.neck = Neck(128, 256, 512)\n",
    "        self.anchors = torch.tensor([[0.08, 0.08], [0.15, 0.15], [0.30, 0.30]])  # (w, h)\n",
    "        # 각 p3, p4, p5 feature map 크기(S), 채널에 맞게 DetectionHead 선언\n",
    "        self.head_p3 = DetectionHead(256, self.anchors, S=64, num_classes=num_classes)\n",
    "        self.head_p4 = DetectionHead(128, self.anchors, S=32, num_classes=num_classes)\n",
    "        self.head_p5 = DetectionHead(256, self.anchors, S=32, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.focus(x)\n",
    "        x = self.conv1(x)\n",
    "        x3 = self.csp1(x)\n",
    "        x = self.conv2(x3)\n",
    "        x4 = self.csp2(x)\n",
    "        x = self.conv3(x4)\n",
    "        x5 = self.spp(x)\n",
    "        p3, p4, p5 = self.neck(x3, x4, x5)\n",
    "        out3 = self.head_p3(p3)\n",
    "        out4 = self.head_p4(p4)\n",
    "        out5 = self.head_p5(p5)\n",
    "        return out3, out4, out5\n",
    "\n",
    "# 8. 테스트 실행\n",
    "if __name__ == '__main__':\n",
    "    # 입력 예시: (batch=1, 3, 256, 256)\n",
    "    x = torch.randn(16, 3, 256, 256)\n",
    "    model = YOLOv5Demo(num_classes=20)  # 예시: VOC 20 클래스\n",
    "    outs = model(x)\n",
    "    print(\"Output shapes:\", [o.shape for o in outs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def voc2yolo(target, img_size, class_to_idx=None):\n",
    "    \"\"\"\n",
    "    VOC XML 딕셔너리(target)를 YOLO 형식 [class_id, x_center, y_center, w, h]로 변환\n",
    "    img_size: (W, H)\n",
    "    class_to_idx: VOC 클래스 이름→인덱스 딕셔너리. 없으면 VOC 기본 20클래스 사용.\n",
    "    \"\"\"\n",
    "    objects = target['annotation']['object']\n",
    "    if not isinstance(objects, list):\n",
    "        objects = [objects]\n",
    "    boxes = []\n",
    "    for obj in objects:\n",
    "        # VOC 클래스 이름\n",
    "        cls_name = obj['name']\n",
    "        # 클래스 인덱스 변환\n",
    "        if class_to_idx is None:\n",
    "            VOC_CLASSES = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
    "                           'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
    "                           'dog', 'horse', 'motorbike', 'person', 'pottedplant',\n",
    "                           'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "            class_to_idx = {name: i for i, name in enumerate(VOC_CLASSES)}\n",
    "        cls_id = class_to_idx[cls_name]\n",
    "        # Bounding box (VOC는 좌상단/우하단 픽셀 좌표)\n",
    "        bndbox = obj['bndbox']\n",
    "        xmin = float(bndbox['xmin'])\n",
    "        ymin = float(bndbox['ymin'])\n",
    "        xmax = float(bndbox['xmax'])\n",
    "        ymax = float(bndbox['ymax'])\n",
    "        w_img, h_img = img_size\n",
    "        # YOLO 형식으로 변환 (center x/y, w/h, 모두 0~1 정규화)\n",
    "        x_center = (xmin + xmax) / 2 / w_img\n",
    "        y_center = (ymin + ymax) / 2 / h_img\n",
    "        box_w = (xmax - xmin) / w_img\n",
    "        box_h = (ymax - ymin) / h_img\n",
    "        boxes.append([cls_id, x_center, y_center, box_w, box_h])\n",
    "    return np.array(boxes, dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008674c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "voc = VOCDetection(root='/media/otter/hard_otter/dataset', year='2012', image_set='trainval')\n",
    "img, target = voc[0]       # img: PIL.Image, target: dict\n",
    "\n",
    "img_tensor = ToTensor()(img)   # (3, H, W)\n",
    "w_img, h_img = img.size\n",
    "\n",
    "yolo_targets = voc2yolo(target, (w_img, h_img))\n",
    "print(\"YOLO targets:\\n\", yolo_targets)\n",
    "# yolo_targets: [ [class_id, x_center, y_center, w, h], ... ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VOCDetectionYOLO(Dataset):\n",
    "    def __init__(self, root, year='2012', image_set='trainval', transforms=None):\n",
    "        self.dataset = VOCDetection(root=root, year=year, image_set=image_set)\n",
    "        self.transforms = transforms\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.dataset[idx]\n",
    "        w, h = img.size\n",
    "        yolo_targets = voc2yolo(target, (w, h))\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img, torch.from_numpy(yolo_targets)\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a4550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def yolo_label_assignment(gt_targets, anchors, S, num_classes, device):\n",
    "    \"\"\"\n",
    "    gt_targets: list of [ [class, x_center, y_center, w, h], ... ] (정규화, 0~1)\n",
    "    anchors: (A, 2)  # anchor box 크기 (w, h), 이미지 스케일 기준\n",
    "    S: output feature map 크기 (SxS)\n",
    "    num_classes: 클래스 개수\n",
    "    device: torch device\n",
    "\n",
    "    반환: (A, S, S, 5+C) tensor\n",
    "    \"\"\"\n",
    "    output = torch.zeros((len(anchors), S, S, 5 + num_classes), device=device)\n",
    "    img_size = 1.0  # GT가 정규화(0~1)이므로, anchor도 정규화값 사용\n",
    "\n",
    "    for gt in gt_targets:\n",
    "        cls, xc, yc, w, h = gt\n",
    "        gt_w, gt_h = w, h  # 0~1 정규화값\n",
    "\n",
    "        # (1) GT 박스 vs 각 anchor IoU 계산 (center=0,0)\n",
    "        ious = []\n",
    "        for anchor in anchors:\n",
    "            anchor_w, anchor_h = anchor[0] / img_size, anchor[1] / img_size\n",
    "            inter_w = min(gt_w, anchor_w)\n",
    "            inter_h = min(gt_h, anchor_h)\n",
    "            inter = inter_w * inter_h\n",
    "            union = gt_w * gt_h + anchor_w * anchor_h - inter\n",
    "            iou = inter / (union + 1e-6)\n",
    "            ious.append(iou)\n",
    "        best_anchor = torch.tensor(ious).argmax().item()\n",
    "\n",
    "        # (2) GT의 center가 포함되는 grid cell 위치\n",
    "        grid_x = int(xc * S)\n",
    "        grid_y = int(yc * S)\n",
    "        # (3) 해당 anchor, cell에 할당\n",
    "        output[best_anchor, grid_y, grid_x, 0:4] = torch.tensor(\n",
    "            [xc * S - grid_x, yc * S - grid_y, w, h], device=device\n",
    "        )  # cell 내 상대 좌표\n",
    "        output[best_anchor, grid_y, grid_x, 4] = 1.0  # objectness\n",
    "        output[best_anchor, grid_y, grid_x, 5 + int(cls)] = 1.0  # class one-hot\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def yolo_simple_loss(pred, target):\n",
    "    \"\"\"\n",
    "    pred/target: (B, A, S, S, 5+C)\n",
    "    \"\"\"\n",
    "    obj_mask = target[..., 4] == 1  # objectness=1인 곳\n",
    "    # box: x, y, w, h\n",
    "    box_loss = ((pred[..., 0:4][obj_mask] - target[..., 0:4][obj_mask]) ** 2).mean()\n",
    "    # objectness: BCE\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    obj_loss = bce(pred[..., 4], target[..., 4])\n",
    "    # class: BCE\n",
    "    if target.shape[-1] > 5:\n",
    "        cls_loss = bce(pred[..., 5:], target[..., 5:])\n",
    "    else:\n",
    "        cls_loss = 0.0\n",
    "    return box_loss + obj_loss + cls_loss\n",
    "\n",
    "S = 13  # feature map 크기\n",
    "anchors = torch.tensor([[0.08, 0.08], [0.15, 0.15], [0.30, 0.30]]).cuda()  # (이미지 대비 w,h), 정규화\n",
    "num_classes = 20\n",
    "in_channels = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be342b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm \n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.transforms import ToTensor\n",
    "voc_yolo = VOCDetectionYOLO(root='/media/otter/hard_otter/dataset', transforms=ToTensor())\n",
    "loader = DataLoader(voc_yolo, batch_size=1, shuffle=True)\n",
    "img, target = voc_yolo[0]\n",
    "print(img.shape)      # torch.Size([3, H, W])\n",
    "print(target.shape)   # (num_objs, 5)\n",
    "\n",
    "model = YOLOv5Demo(20).to('cuda')\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=1e-3,            # (권장 시작값: 1e-3 ~ 1e-2, 후에 줄여도 됨)\n",
    "    momentum=0.9,       # 일반적으로 0.9~0.95 사용\n",
    "    weight_decay=5e-4   # YOLOv1 논문 기본값, 필요 없으면 0으로\n",
    ")\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # 50 epoch마다 lr x0.1\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0    \n",
    "    pbar = tqdm.tqdm(loader)\n",
    "    for i, (imgs, target) in enumerate(pbar):\n",
    "        # 각 이미지별로 dummy GT box 리스트 (class, x_center, y_center, w, h)\n",
    "        # 각 스케일별 label assignment\n",
    "        imgs, targets = imgs.to('cuda'), target.to('cuda')\n",
    "        batch_targets_p3 = torch.stack([\n",
    "            yolo_label_assignment(gt, anchors, S=64, num_classes=num_classes, device=device)\n",
    "            for gt in target\n",
    "        ])\n",
    "        batch_targets_p4 = torch.stack([\n",
    "            yolo_label_assignment(gt, anchors, S=32, num_classes=num_classes, device=device)\n",
    "            for gt in target\n",
    "        ])\n",
    "        batch_targets_p5 = torch.stack([\n",
    "            yolo_label_assignment(gt, anchors, S=32, num_classes=num_classes, device=device)\n",
    "            for gt in target\n",
    "        ])\n",
    "        # Forward\n",
    "        out3, out4, out5 = model(imgs)\n",
    "        # Loss (각 스케일별로)\n",
    "        loss3 = yolo_simple_loss(out3, batch_targets_p3)\n",
    "        loss4 = yolo_simple_loss(out4, batch_targets_p4)\n",
    "        loss5 = yolo_simple_loss(out5, batch_targets_p5)\n",
    "        loss = loss3 + loss4 + loss5\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        total_samples += imgs.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        running_loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss = running_loss / total_samples if total_samples > 0 else 0.0\n",
    "        pbar.desc = f'Epoch {epoch}, Avg Loss: {avg_loss:.4f}'\n",
    "\n",
    "        # 평균 loss\n",
    "        if i%30==0:\n",
    "            img_vis = imgs[0].detach().cpu()\n",
    "            pred_vis = preds[0].detach().cpu()\n",
    "            show_yolo_cv2(img_vis, pred_vis, VOC_CLASSES, winname=\"YOLO Detection\")\n",
    "    loss_history.append(avg_loss)\n",
    "    plot_loss_curve(loss_history)\n",
    "    scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
