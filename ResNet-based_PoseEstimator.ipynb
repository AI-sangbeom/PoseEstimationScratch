{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28401dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.ops import RoIAlign\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNetBackbone(nn.Module):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=pretrained)\n",
    "        # remove avgpool & fc\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.out_channels = 512\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)  # (B, 512, G, G)\n",
    "\n",
    "class AnchorGridDetector(nn.Module):\n",
    "    def __init__(self, in_channels, grid_size=7, anchors=None):\n",
    "        \"\"\"\n",
    "        anchors: List of (w,h) tuples, normalized to [0,1] relative to image size\n",
    "        ex) anchors=[(0.3,0.3), (0.6,0.6), (1.0,1.0)]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.G = grid_size\n",
    "        self.anchors = torch.tensor(anchors, dtype=torch.float32)  # (A,2)\n",
    "        self.A = len(anchors)\n",
    "        # A×5 channels: tx, ty, tw, th, obj_conf\n",
    "        self.head = nn.Conv2d(in_channels, self.A*5, kernel_size=1)\n",
    "\n",
    "    def forward(self, fmap):\n",
    "        B, C, G, _ = fmap.shape\n",
    "        device = fmap.device\n",
    "\n",
    "        # 1) raw predictions\n",
    "        p = self.head(fmap)                          # (B, A*5, G, G)\n",
    "        p = p.view(B, self.A, 5, G, G)               # (B, A, 5, G, G)\n",
    "        p = p.permute(0, 3, 4, 1, 2)                 # (B, G, G, A, 5)\n",
    "\n",
    "        # 2) decode (optional, inference 때만)\n",
    "        conf = torch.sigmoid(p[..., 4])              # (B,G,G,A)\n",
    "        xy   = torch.sigmoid(p[..., 0:2])            # offsets\n",
    "        wh   = torch.exp(p[..., 2:4]) * self.anchors.view(1,1,1,self.A,2)\n",
    "\n",
    "        # grid cell 좌표\n",
    "        grid = torch.arange(G, device=device, dtype=torch.float32)\n",
    "        gy, gx = torch.meshgrid(grid, grid, indexing='ij')\n",
    "        gx = gx.view(1,G,G,1); gy = gy.view(1,G,G,1)\n",
    "        cell = 1.0 / G\n",
    "\n",
    "        x_abs = (gx + xy[...,0:1]) * cell\n",
    "        y_abs = (gy + xy[...,1:2]) * cell\n",
    "        w_abs = wh[...,0:1] * cell\n",
    "        h_abs = wh[...,1:2] * cell\n",
    "\n",
    "        # inference 시 boxes 리스트로 뽑으려면 위 값을 이용하세요...\n",
    "        return p, (x_abs, y_abs, w_abs, h_abs, conf)\n",
    "    \n",
    "\n",
    "class PoseHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_keypoints=4, pool_size=7):\n",
    "        super().__init__()\n",
    "        self.roi_align = RoIAlign((pool_size, pool_size),\n",
    "                                  spatial_scale=1.0,  # 이미 normalized coords 사용\n",
    "                                  sampling_ratio=-1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels * pool_size * pool_size, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_keypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, fmap, boxes):\n",
    "        \"\"\"\n",
    "        fmap: (B, C, H, W)\n",
    "        boxes: List[Tensor(N_i, 4)] in normalized coords [x1,y1,x2,y2]\n",
    "        \"\"\"\n",
    "        # concat all boxes with batch idx\n",
    "        rois = []\n",
    "        for b_idx, b in enumerate(boxes):\n",
    "            if b.numel() == 0:\n",
    "                continue\n",
    "            idx = torch.full((b.size(0),1), b_idx, device=fmap.device)\n",
    "            rois.append(torch.cat([idx, b], dim=1))\n",
    "        if not rois:\n",
    "            return torch.empty((0, self.fc[-1].out_features), device=fmap.device)\n",
    "        rois = torch.cat(rois, dim=0)  # (sum_N, 5)\n",
    "        aligned = self.roi_align(fmap, rois)  # (sum_N, C, pool, pool)\n",
    "        flat = self.flatten(aligned)          # (sum_N, C*pool*pool)\n",
    "        coords = self.fc(flat)                # (sum_N, K*2)\n",
    "        return coords\n",
    "\n",
    "class MultiObjectKeypointNet(nn.Module):\n",
    "    def __init__(self, num_keypoints=4, grid_size=7, pretrained_backbone=False):\n",
    "        super().__init__()\n",
    "        self.backbone = ResNetBackbone(pretrained=pretrained_backbone)\n",
    "        self.detector = AnchorGridDetector(self.backbone.out_channels, grid_size)\n",
    "        self.pose_head = PoseHead(self.backbone.out_channels, num_keypoints)\n",
    "        self.grid_size = grid_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B,3,H,W), expected normalized to [0,1]\n",
    "        returns:\n",
    "          boxes_batch: List[Tensor(N_i,4)] normalized bbox coords\n",
    "          keypoints_batch: List[List[Tensor(K*2)]] per-object keypoints\n",
    "        \"\"\"\n",
    "        B, _, H, W = x.shape\n",
    "        fmap = self.backbone(x)\n",
    "        boxes_batch, scores = self.detector(fmap)\n",
    "        coords = self.pose_head(fmap, boxes_batch)  # (sum_N, K*2)\n",
    "\n",
    "        # split coords back per image\n",
    "        keypoints_batch = [[] for _ in range(B)]\n",
    "        ptr = 0\n",
    "        for b_idx, boxes in enumerate(boxes_batch):\n",
    "            n = boxes.size(0)\n",
    "            if n>0:\n",
    "                keypoints_batch[b_idx] = coords[ptr:ptr+n]\n",
    "            ptr += n\n",
    "\n",
    "        return boxes_batch, keypoints_batch\n",
    "    \n",
    "    def training_forward(self, x, det_labels, pose_labels):\n",
    "        B = x.size(0)\n",
    "        fmap = self.backbone(x)\n",
    "        # 1) Detection raw\n",
    "        p, _ = self.detector(fmap)                   # (B,G,G,A,5)\n",
    "        G, A = self.G, self.detector.A\n",
    "        device = x.device\n",
    "\n",
    "        # 2) 타깃 생성\n",
    "        #   det_target: (B,G,G,A,5) \n",
    "        #   ignore_mask: (B,G,G,A) → no-object 학습 시 제외할 것\n",
    "        det_target    = torch.zeros_like(p, device=device)\n",
    "        ignore_mask   = torch.zeros((B,G,G,A), device=device, dtype=torch.bool)\n",
    "\n",
    "        for b in range(B):\n",
    "            boxes = det_labels[b]    # (N_i,5): [conf,x,y,w,h]\n",
    "            # gt corner 계산\n",
    "            gt_xywh = boxes[:,1:5]  # (N_i,4)\n",
    "            # 각 gt마다 best anchor 찾기\n",
    "            for gt in gt_xywh:\n",
    "                x_c,y_c,w,h = gt.tolist()\n",
    "                gj = int(x_c * G); gi = int(y_c * G)\n",
    "\n",
    "                # anchor별 IoU(wh vs anchor_wh) 계산\n",
    "                # 단순히 wh비교 -> min/max ratio로 대체 가능\n",
    "                anchor_wh = self.detector.anchors.to(device)  # (A,2)\n",
    "                gt_wh     = gt[2:4].unsqueeze(0)              # (1,2)\n",
    "                min_wh    = torch.min(anchor_wh, gt_wh)\n",
    "                max_wh    = torch.max(anchor_wh, gt_wh)\n",
    "                ious_wh   = (min_wh.prod(dim=1) / max_wh.prod(dim=1))  # (A,)\n",
    "                best_a    = torch.argmax(ious_wh)\n",
    "\n",
    "                # objectness target\n",
    "                det_target[b, gi, gj, best_a, 4] = 1.0\n",
    "\n",
    "                # tx, ty : cell 내 상대 offset\n",
    "                det_target[b, gi, gj, best_a, 0] = x_c*G - gj\n",
    "                det_target[b, gi, gj, best_a, 1] = y_c*G - gi\n",
    "\n",
    "                # tw, th : log-space ratio\n",
    "                aw, ah = anchor_wh[best_a]\n",
    "                det_target[b, gi, gj, best_a, 2] = torch.log(w/aw + 1e-6)\n",
    "                det_target[b, gi, gj, best_a, 3] = torch.log(h/ah + 1e-6)\n",
    "\n",
    "                # 나머지 anchor들은 no-object로 학습하되, \n",
    "                # GT 박스와 IoU가 작은 anchor만 no-object loss에 포함\n",
    "                ignore_mask[b, gi, gj, :] = ious_wh > 0.5\n",
    "\n",
    "        # 3) Pose GT 박스 준비 (corner)\n",
    "        gt_corner_boxes = []\n",
    "        for b in range(B):\n",
    "            corners = []\n",
    "            for det in det_labels[b]:\n",
    "                _, x_c, y_c, w, h = det.tolist()\n",
    "                x1,y1 = x_c - w/2, y_c - h/2\n",
    "                x2,y2 = x_c + w/2, y_c + h/2\n",
    "                corners.append([x1, y1, x2, y2])\n",
    "            if corners:\n",
    "                gt_corner_boxes.append(torch.tensor(corners, device=device))\n",
    "            else:\n",
    "                gt_corner_boxes.append(torch.empty((0,4), device=device))\n",
    "\n",
    "        # 4) Pose raw\n",
    "        pose_pred = self.pose_head(fmap, gt_corner_boxes)  # (sum_N, K*2)\n",
    "        pose_target = torch.cat(pose_labels, dim=0) if pose_labels else torch.empty_like(pose_pred)\n",
    "\n",
    "        return p, det_target, ignore_mask, pose_pred, pose_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0361101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# (기존 클래스 import 후)\n",
    "\n",
    "# Custom Dataset 정의\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.img_dir = os.path.join(data_path, 'images')\n",
    "        self.label_dir = os.path.join(data_path, 'labels')\n",
    "        self.img_files = sorted(os.listdir(self.img_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.img_files[idx]\n",
    "        label_file = img_file.replace('.jpg', '.txt')\n",
    "\n",
    "        # 이미지 로드\n",
    "        img_path = os.path.join(self.img_dir, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # 레이블 로드 (모든 객체)\n",
    "        label_path = os.path.join(self.label_dir, label_file)\n",
    "        det_labels = []\n",
    "        pose_labels = []\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    det = [1.0] + [float(x) for x in parts[1:5]]  # objectness = 1.0\n",
    "                    pose = [float(x) for x in parts[5:]]\n",
    "                    det_labels.append(det)\n",
    "                    pose_labels.append(pose)\n",
    "\n",
    "        det_labels = torch.tensor(det_labels, dtype=torch.float32)  # (num_objs, 5)\n",
    "        pose_labels = torch.tensor(pose_labels, dtype=torch.float32)  # (num_objs, num_kps*2)\n",
    "\n",
    "        return img, det_labels, pose_labels\n",
    "\n",
    "\n",
    "    \n",
    "base = '/home/otter/dataset/pallet/dataset/train'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((360, 360)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "def custom_collate_fn(batch):\n",
    "    images, dets, poses = zip(*batch)  # 튜플을 분리\n",
    "\n",
    "    images = torch.stack(images, dim=0)  # 이미지만은 고정 크기이므로 stack 가능\n",
    "\n",
    "    return images, list(dets), list(poses)\n",
    "dataset = CustomDataset(base, transform)\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiObjectKeypointNet(num_keypoints=4, grid_size=7, pretrained_backbone=False)\n",
    "model.to(device)\n",
    "\n",
    "# Detection: objectness은 BCEWithLogits, bbox는 MSE\n",
    "det_loss_fn_conf = nn.BCEWithLogitsLoss()\n",
    "det_loss_fn_bbox = nn.MSELoss()\n",
    "pose_loss_fn     = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# --- 학습 루프 ---\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_det_loss  = 0.0\n",
    "    total_pose_loss = 0.0\n",
    "\n",
    "    for imgs, det_labels, pose_labels in tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        # imgs: Tensor(B,3,H,W)\n",
    "        # det_labels: List of Tensor(N_i,4)\n",
    "        # pose_labels: List of Tensor(N_i,K*2)\n",
    "        imgs = imgs.to(device)\n",
    "        det_labels  = [d.to(device) for d in det_labels]\n",
    "        pose_labels = [p.to(device) for p in pose_labels]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        det_out, det_target, pose_out, pose_target = \\\n",
    "            model.training_forward(imgs, det_labels, pose_labels)\n",
    "\n",
    "        # Detection loss\n",
    "        conf_pred   = det_out[..., 0]\n",
    "        conf_target = det_target[..., 0]\n",
    "        bbox_pred   = det_out[..., 1:]\n",
    "        bbox_target = det_target[..., 1:]\n",
    "        loss_conf = det_loss_fn_conf(conf_pred,   conf_target)\n",
    "        loss_bbox = det_loss_fn_bbox(bbox_pred, bbox_target)\n",
    "        det_loss = loss_conf + loss_bbox\n",
    "\n",
    "        # Pose loss\n",
    "        pose_loss = pose_loss_fn(pose_out, pose_target)\n",
    "\n",
    "        # Backprop & step\n",
    "        loss = det_loss + pose_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_det_loss  += det_loss.item()\n",
    "        total_pose_loss += pose_loss.item()\n",
    "\n",
    "    avg_det  = total_det_loss  / len(loader)\n",
    "    avg_pose = total_pose_loss / len(loader)\n",
    "    print(f\"[Epoch {epoch+1}] det_loss: {avg_det:.4f} | pose_loss: {avg_pose:.4f}\")\n",
    "print(\"학습 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35763c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'checkpoints/resnet_based_pe.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f27daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# 디바이스 설정 (CUDA 사용 가능하면 GPU, 아니면 CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 1) 모델 로드 ---\n",
    "# 모델 정의가 있는 파일에서 MultiObjectKeypointNet 클래스를 import 해두세요.\n",
    "# from your_model_file import MultiObjectKeypointNet\n",
    "\n",
    "model = torch.load('checkpoints/resnet_based_pe.pt', map_location=device)\n",
    "model.to(device).eval()  # 평가 모드로 전환\n",
    "\n",
    "# --- 2) 입력 전처리 정의 ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((360, 360)),             # 모델 입력 크기에 맞춰 리사이즈\n",
    "    transforms.ToTensor(),                     # Tensor로 변환 (C×H×W, [0,1])\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std =[0.229,0.224,0.225]),  # ImageNet 표준 정규화\n",
    "])\n",
    "\n",
    "# --- 3) 비디오 폴더의 .mp4 파일 순회 ---\n",
    "video_dir = '/home/otter/workspace/Pallet/train_video'\n",
    "video_files = sorted([\n",
    "    os.path.join(video_dir, f)\n",
    "    for f in os.listdir(video_dir)\n",
    "    if f.lower().endswith('.mp4')\n",
    "])\n",
    "\n",
    "for video_path in video_files:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"[경고] 비디오 열기 실패: {video_path}\")\n",
    "        continue\n",
    "\n",
    "    while True:\n",
    "        ret, frame_bgr = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        H, W = frame_bgr.shape[:2]\n",
    "\n",
    "        # OpenCV BGR → RGB → PIL → transform → 배치 차원 추가\n",
    "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "        pil_img    = Image.fromarray(frame_rgb)\n",
    "        inp        = transform(pil_img).unsqueeze(0).to(device)  # (1,3,360,360)\n",
    "\n",
    "        # 추론\n",
    "        with torch.no_grad():\n",
    "            boxes_batch, kps_batch = model(inp)\n",
    "\n",
    "        # 배치 크기 1 이므로 첫 번째 결과만 사용\n",
    "        boxes = boxes_batch[0]  # Tensor(num_objs, 4) — [x1,y1,x2,y2] (정규화)\n",
    "        kps   = kps_batch[0]    # Tensor(num_objs, K*2) — 박스 내 상대좌표\n",
    "\n",
    "        # 각 객체마다 박스와 키포인트를 원본 해상도로 변환해 그리기\n",
    "        for obj_idx in range(boxes.shape[0]):\n",
    "            x1n, y1n, x2n, y2n = boxes[obj_idx].tolist()\n",
    "            # 정규화된 좌표 → 픽셀 좌표\n",
    "            x1, y1 = int(x1n * W), int(y1n * H)\n",
    "            x2, y2 = int(x2n * W), int(y2n * H)\n",
    "\n",
    "            # 키포인트: 상대좌표 → 절대좌표\n",
    "            obj_kps = kps[obj_idx]  # (K*2,)\n",
    "            xs = obj_kps[0::2] * (x2 - x1) + x1\n",
    "            ys = obj_kps[1::2] * (y2 - y1) + y1\n",
    "            xs = xs.cpu().numpy().astype(int)\n",
    "            ys = ys.cpu().numpy().astype(int)\n",
    "\n",
    "            for (kx, ky) in zip(xs, ys):\n",
    "                cv2.circle(frame_bgr, (kx, ky), 4, (0,0,255), -1)\n",
    "\n",
    "        # 결과 프레임 출력  \n",
    "        cv2.imshow(\"Inference\", frame_bgr)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):  # 'q' 누르면 종료\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
