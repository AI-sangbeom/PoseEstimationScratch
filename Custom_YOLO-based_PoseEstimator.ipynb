{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63c7d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "\n",
    "class SimpleDetectorPoseEstimator(nn.Module):\n",
    "    def __init__(self, num_keypoints=4, grid_size=7, img_size=224):\n",
    "        super(SimpleDetectorPoseEstimator, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        # YOLO-like Object Detector\n",
    "        self.detector = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(grid_size),\n",
    "            nn.Conv2d(256, 5, kernel_size=1)  # [obj_conf, x, y, w, h]\n",
    "        )\n",
    "\n",
    "        # Pose Estimator\n",
    "        self.pose_estimator = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, num_keypoints * 2)\n",
    "        )\n",
    "\n",
    "\n",
    "    def detect_objects(self, image_tensor, conf_threshold=0.5):\n",
    "        B, _, _, _ = image_tensor.shape\n",
    "        predictions = self.detector(image_tensor).permute(0, 2, 3, 1)  # (B, G, G, 5)\n",
    "\n",
    "        obj_conf = torch.sigmoid(predictions[..., 0])\n",
    "        x = torch.sigmoid(predictions[..., 1])\n",
    "        y = torch.sigmoid(predictions[..., 2])\n",
    "        w = torch.sigmoid(predictions[..., 3])\n",
    "        h = torch.sigmoid(predictions[..., 4])\n",
    "\n",
    "        grid_range = torch.arange(self.grid_size, dtype=torch.float32, device=image_tensor.device)\n",
    "        gy, gx = torch.meshgrid(grid_range, grid_range, indexing='ij')\n",
    "        gx = gx.unsqueeze(0)  # (1, G, G)\n",
    "        gy = gy.unsqueeze(0)  # (1, G, G)\n",
    "\n",
    "        cell_size = 1.0 / self.grid_size\n",
    "        x_abs = (gx + x) * cell_size  # (B, G, G)\n",
    "        y_abs = (gy + y) * cell_size\n",
    "\n",
    "        w_abs = w\n",
    "        h_abs = h\n",
    "\n",
    "        mask = obj_conf > conf_threshold  # (B, G, G)\n",
    "        boxes_batch = []\n",
    "        for b in range(B):\n",
    "            mask_b = mask[b]\n",
    "            x_b = x_abs[b][mask_b]\n",
    "            y_b = y_abs[b][mask_b]\n",
    "            w_b = w_abs[b][mask_b]\n",
    "            h_b = h_abs[b][mask_b]\n",
    "            boxes = torch.stack([x_b, y_b, w_b, h_b], dim=1)  # (N, 4)\n",
    "            boxes_batch.append(boxes.tolist())\n",
    "\n",
    "        return boxes_batch  # List of List[box]\n",
    "\n",
    "    def crop_objects(self, images, boxes_batch, margin=0.1):\n",
    "        crops = []\n",
    "        for img_idx, (image, boxes) in enumerate(zip(images, boxes_batch)):\n",
    "            H, W, _ = image.shape\n",
    "            for x_c, y_c, w, h in boxes:\n",
    "                w_new = w * (1 + margin)\n",
    "                h_new = h * (1 + margin)\n",
    "\n",
    "                x1 = int((x_c - w_new / 2) * W)\n",
    "                y1 = int((y_c - h_new / 2) * H)\n",
    "                x2 = int((x_c + w_new / 2) * W)\n",
    "                y2 = int((y_c + h_new / 2) * H)\n",
    "\n",
    "                x1 = max(0, x1)\n",
    "                y1 = max(0, y1)\n",
    "                x2 = min(W, x2)\n",
    "                y2 = min(H, y2)\n",
    "\n",
    "                crop = image[y1:y2, x1:x2]\n",
    "                crop_resized = cv2.resize(crop, (self.img_size, self.img_size))\n",
    "                crops.append(crop_resized)\n",
    "\n",
    "        return crops\n",
    "\n",
    "    def estimate_pose(self, cropped_images):\n",
    "        if not cropped_images:\n",
    "            return []\n",
    "\n",
    "        crop_tensors = torch.stack([\n",
    "            torch.tensor(crop, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "            for crop in cropped_images\n",
    "        ])\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_extractor(crop_tensors)\n",
    "            keypoints = self.pose_estimator(features)\n",
    "        return keypoints.cpu().numpy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        det_output = self.detector(features).permute(0, 2, 3, 1)  # (B, G, G, 5)\n",
    "        center = self.grid_size // 2\n",
    "        det_preds = det_output[:, center, center, :]  # (B, 5)\n",
    "\n",
    "        pose_preds = self.pose_estimator(features)  # (B, K*2)\n",
    "\n",
    "        return det_preds, pose_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a931d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:51<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Det Loss: 0.2252, Pose Loss: 0.2167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:51<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Det Loss: 0.1198, Pose Loss: 0.1003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:50<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Det Loss: 0.0667, Pose Loss: 0.0509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:52<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Det Loss: 0.0397, Pose Loss: 0.0338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:52<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Det Loss: 0.0262, Pose Loss: 0.0291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:51<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Det Loss: 0.0200, Pose Loss: 0.0281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:52<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] Det Loss: 0.0174, Pose Loss: 0.0278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:52<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] Det Loss: 0.0164, Pose Loss: 0.0277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:51<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] Det Loss: 0.0160, Pose Loss: 0.0275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:52<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Det Loss: 0.0156, Pose Loss: 0.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:52<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Det Loss: 0.0154, Pose Loss: 0.0272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:51<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Det Loss: 0.0151, Pose Loss: 0.0270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:52<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Det Loss: 0.0148, Pose Loss: 0.0268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:57<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] Det Loss: 0.0145, Pose Loss: 0.0266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [01:00<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] Det Loss: 0.0142, Pose Loss: 0.0264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [01:00<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Det Loss: 0.0140, Pose Loss: 0.0262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:58<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] Det Loss: 0.0137, Pose Loss: 0.0260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:59<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] Det Loss: 0.0134, Pose Loss: 0.0258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:59<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] Det Loss: 0.0132, Pose Loss: 0.0257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:54<00:00,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] Det Loss: 0.0129, Pose Loss: 0.0255\n",
      "학습 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# (기존 클래스 import 후)\n",
    "\n",
    "# Custom Dataset 정의\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.img_dir = os.path.join(data_path, 'images')\n",
    "        self.label_dir = os.path.join(data_path, 'labels')\n",
    "        self.img_files = sorted(os.listdir(self.img_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.img_files[idx]\n",
    "        label_file = img_file.replace('.jpg', '.txt')\n",
    "\n",
    "        # 이미지 로드\n",
    "        img_path = os.path.join(self.img_dir, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # 레이블 로드 (모든 객체)\n",
    "        label_path = os.path.join(self.label_dir, label_file)\n",
    "        det_labels = []\n",
    "        pose_labels = []\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    det = [1.0] + [float(x) for x in parts[1:5]]  # objectness = 1.0\n",
    "                    pose = [float(x) for x in parts[5:]]\n",
    "                    det_labels.append(det)\n",
    "                    pose_labels.append(pose)\n",
    "\n",
    "        det_labels = torch.tensor(det_labels, dtype=torch.float32)  # (num_objs, 5)\n",
    "        pose_labels = torch.tensor(pose_labels, dtype=torch.float32)  # (num_objs, num_kps*2)\n",
    "\n",
    "        return img, det_labels, pose_labels\n",
    "\n",
    "    \n",
    "base = '/home/otter/dataset/pallet/dataset/train'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((360, 360)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "def custom_collate_fn(batch):\n",
    "    images, dets, poses = zip(*batch)  # 튜플을 분리\n",
    "\n",
    "    images = torch.stack(images, dim=0)  # 이미지만은 고정 크기이므로 stack 가능\n",
    "\n",
    "    return images, list(dets), list(poses)\n",
    "dataset = CustomDataset(base, transform)\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Loss 정의\n",
    "det_loss_fn = nn.MSELoss()\n",
    "pose_loss_fn = nn.MSELoss()\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = SimpleDetectorPoseEstimator(num_keypoints=4).to(device)\n",
    "\n",
    "# Optimizer 정의\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.detector.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.pose_estimator.parameters(), 'lr': 1e-4}\n",
    "])\n",
    "\n",
    "# 학습 루프\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    total_det_loss, total_pose_loss = 0, 0\n",
    "    pbar = tqdm(loader)\n",
    "    for imgs, det_labels, pose_labels in pbar:\n",
    "        imgs = imgs.to(device)\n",
    "        # det_labels = det_labels.to(device)\n",
    "        # pose_labels = pose_labels.to(device)\n",
    "        det_labels = torch.stack([\n",
    "            det[0] if len(det) > 0 else torch.zeros(5) for det in det_labels\n",
    "        ]).to(device)\n",
    "        pose_labels = torch.stack([\n",
    "            det[0] if len(det) > 0 else torch.zeros(4) for det in pose_labels\n",
    "        ]).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        det_preds, pose_preds = model(imgs)\n",
    "        det_loss = det_loss_fn(det_preds, det_labels)\n",
    "        pose_loss = pose_loss_fn(pose_preds, pose_labels)\n",
    "\n",
    "        loss = det_loss + pose_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_det_loss += det_loss.item()\n",
    "        total_pose_loss += pose_loss.item()\n",
    "        # color_frame = imgs[0]\n",
    "        # pred = model.pose_estimator(color_frame.unsqueeze(0))\n",
    "        \n",
    "        # color_frame = color_frame.permute(1, 2, 0).detach().cpu().numpy()\n",
    "        # xs = pred[:, 0::2] * 360\n",
    "        # ys = pred[:, 1::2] * 360\n",
    "        # for k in range(len(xs)):\n",
    "        #     x = xs[k].type(torch.int32).detach().cpu().numpy()\n",
    "        #     y = ys[k].type(torch.int32).detach().cpu().numpy()\n",
    "        #     for i in range(len(x)):\n",
    "        #         cv2.circle(color_frame, (x, y), 5, (0, 0, 255), -1)\n",
    "        # cv2.imshow('frame', color_frame)    \n",
    "        # key = cv2.waitKey(1) & 0xFF\n",
    "        # if key == ord('q') or key == ord('p'): break\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] Det Loss: {total_det_loss/len(loader):.4f}, Pose Loss: {total_pose_loss/len(loader):.4f}\")\n",
    "\n",
    "print(\"학습 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b874f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'checkpoints/ype.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0459eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_246967/1313506405.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('checkpoints/ype.pt').cuda()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision import transforms\n",
    "model = torch.load('checkpoints/ype.pt').cuda()\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((360, 360)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 모델 사용 예시\n",
    "file_base = '/home/otter/workspace/Pallet/train_video'\n",
    "files_list = os.listdir(file_base)\n",
    "files = [os.path.join(file_base, file) for file in files_list if file.endswith('.mp4')]\n",
    "for file in files:\n",
    "    key = None\n",
    "    cap = cv2.VideoCapture(file)\n",
    "    while cap.isOpened():\n",
    "        ret, color_frame = cap.read()\n",
    "        if ret: \n",
    "            input_frame = transform(color_frame).to('cuda').unsqueeze(0)\n",
    "            det_preds, outputs = model(input_frame)\n",
    "            xs = outputs[:, 0::2] * 1920\n",
    "            ys = outputs[:, 1::2] * 1080\n",
    "            for k in range(len(xs)):\n",
    "                x = xs[k].type(torch.int32).detach().cpu().numpy()\n",
    "                y = ys[k].type(torch.int32).detach().cpu().numpy()\n",
    "                for i in range(len(x)):\n",
    "                    cv2.circle(color_frame, (x[i], y[i]), 5, (0, 0, 255), -1)\n",
    "            cv2.imshow('frame', color_frame)    \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q') or key == ord('p'): break\n",
    "        else:\n",
    "            break \n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
